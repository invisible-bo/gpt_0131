## Prompt truncation

AI가 처리할 수 있는 길이를 초과하면
프롬프트 일부(보통 앞부분)가 잘리는 것

- 입력 길이가 너무 길 때
- 모델의 토큰 제한 초과

엄청 긴 질문을 던졌는데, 앞부분이 날아가서 AI가 제대로 이해 못 함

## Context loss

AI가 대화 맥락이나 중요한 정보를 기억하지 못하는 현상

- 이전 대화가 너무 길어서 삭제됨
- 세션이 종료되어 AI가 정보를 잊음

앞에서 "이거 기억해"라고 했는데, 후반부에 가서 AI가 까먹음

---

Prompt truncation : 입력할 때 잘리는 것 (던진 문장의 일부가 날아감)

Context loss : 대화하면서 정보가 사라지는 거. (기억해야 할 것 까먹음)

**트렁케이션**으로 인해 중요한 정보가 사라지면 → 대화 맥락이 끊기고 **컨텍스트 손실**이 발생

---
## CoT (Chain-of-Thought) 
- AI가 단계 별로 논리적인 사고 과정을 거쳐 답을 도출하는 방식

- 그냥 정답을 내뱉는 게 아니라, 중간 과정까지 생각하면서 답을 내는 것

- AI들은 질문을 받으면 바로 정답을 예측하려고 했다.
하지만 CoT 기법을 사용하면, AI가 스스로 중간 논리를 생각하면서 더 정확한 답을 낼 수 있음

### CoT의 핵심 포인트
1. 사고 과정이 명확해짐 → AI가 더 논리적인 답을 낼 수 있음

2. 복잡한 문제 해결 가능 → 단순 계산이 아니라 추론, 논리, 응용 문제도 가능

3. 신뢰성이 높아짐 → "왜 이렇게 답을 내렸는지" 설명 가능

**관련논문**
[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)
사고 사슬 유도는 대규모 언어 모델에서 추론을 이끌어냅니다

요약 번역:
우리는 생각의 사슬(일련의 중간 추론 단계)을 생성하여 복잡한 추론을 수행하는 대규모 언어 모델의 능력을 어떻게 크게 향상시키는지 탐구합니다. 
특히, 우리는 몇 가지 사고 연쇄 시연이 프롬프트의 예시로 제공되는 사고 연쇄 프롬프트라는 간단한 방법을 통해 충분히 큰 언어 모델에서 그러한 추론 능력이 어떻게 자연스럽게 나타나는지 보여줍니다. 
세 가지 대규모 언어 모델에 대한 실험에서는 일련의 사고 유도가 다양한 산술, 상식 및 상징적 추론 작업의 성능을 향상시키는 것으로 나타났습니다. 
경험적 이득은 놀라울 수 있습니다. 예를 들어, 단지 8개의 사고 예시 체인을 사용하여 
540B 매개변수 언어 모델을 실행하면 수학 단어 문제의 GSM8K 벤치마크에서 최첨단 정확도를 얻을 수 있으며 검증기로 미세 조정된 GPT-3도 능가합니다.

[Faithful Chain-of-Thought Reasoning](https://arxiv.org/abs/2301.13379)
충실한 사고 사슬 추론

요약 번역:
CoT(사고 사슬) 프롬프트는 복잡한 추론 작업 전반에 걸쳐 언어 모델(LM) 성능을 향상시키지만, 
생성된 추론 체인은 모델이 답변(일명 충실성)에 도달하는 방식을 반드시 반영하지는 않습니다. 
우리는 각각 LM과 결정론적 솔버를 사용하여 번역(자연어 쿼리 → 기호 추론 체인) 및 문제 해결(추론 체인 → 답변)의 두 단계로 구성된 추론 프레임워크인 Faithful CoT를 제안합니다. 
이는 추론 체인이 최종 답에 대한 충실한 설명을 제공한다는 것을 보장합니다. 
해석 가능성 외에도 Faithful CoT는 실증적 성능도 향상시킵니다. 
4가지 다양한 영역의 10개 벤치마크 중 9개에서 표준 CoT를 능가하며 MWP(수학 단어 문제)에서 6.3%, 계획에서 3.4%, 다중에서 5.5%의 상대 정확도 향상을 보였습니다. 
-hop QA(질문 응답), 관계형 추론에서 21.4%. 또한 GPT-4 및 Codex를 통해 
7개 데이터세트(6개 데이터세트에서 95.0+ 정확도)에 대한 새로운 최첨단 퓨샷 성능을 설정하여 
충실도와 정확성 사이의 강력한 시너지 효과를 보여줍니다.

---

## RAG (Retrieval-Augmented Generation, 검색 증강 생성)
- AI가 기억력 만으로(단순히 학습된 지식) 답을 생성하는 게 아니라, 최신 정보를 검색해서 
더 정확한 답을 만들어내는 방식

### RAG의 작동 방식
1. 검색 (Retrieval)

- AI가 자체 훈련 데이터에 의존하지 않고, 외부 데이터베이스나 검색 엔진에서 최신 정보를 가져옴

- 예를 들어, 위키 백과, 논문, 문서, API, 뉴스 기사 등을 검색해서 필요한 정보를 찾음

2. 생성 (Generation)

- 검색한 정보를 바탕으로 답변을 생성

- AI가 검색한 내용을 이해하고, 그걸 자연스럽게 풀어서 대답

**관련논문**

[A Survey on Retrieval-Augmented Text Generation](https://arxiv.org/abs/2202.01110)
검색-증강 텍스트 생성에 관한 설문조사

요약 번역:
최근 검색을 통해 증강된 텍스트 생성이 컴퓨터 언어학 커뮤니티의 관심을 끌고 있습니다. 
기존 생성 모델과 비교하여 검색 증강 텍스트 생성은 놀라운 이점을 가지며 특히 많은 NLP 작업에서 최첨단 성능을 달성했습니다. 
본 논문은 검색증강 텍스트 생성에 관한 조사를 실시하는 것을 목표로 한다. 
먼저 검색 증강 생성의 일반적인 패러다임을 강조한 다음 대화 응답 생성, 기계 번역 및 기타 생성 작업을 포함한 다양한 작업에 따라 주목할만한 접근 방식을 검토합니다. 
마지막으로, 향후 연구를 촉진하기 위한 최근 방법 외에 몇 가지 중요한 방향을 제시합니다.

[Searching for Best Practices in Retrieval-Augmented Generation](https://aclanthology.org/2024.emnlp-main.981/)
검색 증강 생성의 모범 사례 검색

요약 번역:
검색 증강 생성(RAG) 기술은 특히 전문 영역에서 최신 정보를 통합하고 환각을 완화하며 응답 품질을 향상시키는 데 효과적인 것으로 입증되었습니다. 
쿼리 종속 검색을 통해 대규모 언어 모델을 향상시키기 위해 많은 RAG 접근 방식이 제안되었지만 이러한 접근 방식은 여전히 ​​복잡한 구현과 긴 응답 시간으로 인해 어려움을 겪고 있습니다. 
일반적으로 RAG 워크플로에는 여러 처리 단계가 포함되며 각 단계는 다양한 방식으로 실행될 수 있습니다. 여기서는 최적의 RAG 방식을 식별하기 위해 기존 RAG 접근 방식과 잠재적 조합을 조사합니다. 
광범위한 실험을 통해 성능과 효율성의 균형을 유지하는 RAG 배포를 위한 몇 가지 전략을 제안합니다. 
또한, 우리는 다중 모드 검색 기술이 시각적 입력에 대한 질문 답변 기능을 크게 향상시키고 
"생성으로서의 검색" 전략을 사용하여 다중 모드 콘텐츠 생성을 가속화할 수 있음을 보여줍니다.